{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c25ef4",
   "metadata": {},
   "source": [
    "# Welcome to our Project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81e533",
   "metadata": {},
   "source": [
    "## Goal of This Project: \n",
    "### We are continuing working on using regression modeling to make predictions based on our Zillow data of 2017 properties for single family homes. In this itteration of the project we are using clustering algorithms to aid us in determining similarites of the data. We are also shifting our focus to logerror rather than tax value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1eaa18",
   "metadata": {},
   "source": [
    "## Project Plan\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a996d",
   "metadata": {},
   "source": [
    "### Acquire and Prepare Data\n",
    "   1. Data is brought in from either the SQL database using a locally stored env file that must contain the username, password, and host name in a get_bd_url function OR via a locally strored csv file.\n",
    "   2. Data is prepared in order for our team to explore and modified to use in modeling\n",
    "   3. Data is split into three dataframes called train, validate, and test so that we can train and explore on in-sample data and use out-of-sample to validate and finally test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfc78f",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "   1. Use visualizations to see patterns in the data and try to determine drivers/indicators of logerror\n",
    "   2. Use statistical testing, if necessary, to confirm or reject the relationship between features\n",
    "   3. Use clustering to create new categorical features that can be explored \n",
    "   4. Determine what features align with logerror deviating significantly from zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf2972",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "   1. Take features determined to be significant in exploration\n",
    "   2. Determine which regression algorithm to use\n",
    "   3. Run 4 models with chosen features and determine the best one by evaluating the RMSE score on the train set and run on the validate set\n",
    "   4. The best model will be deterined by lowest RMSE on train, and lowest variance between the RMSE of train and validate\n",
    "   5. Run the top single model on the test dataset\n",
    "   6. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ee02b",
   "metadata": {},
   "source": [
    "### Recommendations / What Comes Next\n",
    "   1. Based on model evaluation of the test dataset make recommendations and predictions\n",
    "   2. Recommendations will include...\n",
    "   3. Further steps are..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24196ffa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle as w\n",
    "import explore as e\n",
    "import wrangle_Naomi as wn\n",
    "import model as m\n",
    "import env\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from importlib import reload\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9551f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add *\n",
    "! git commit -m \"[Insert Commit Message Here]\"\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99c2fd",
   "metadata": {},
   "source": [
    "## Wrangling the Data (from SQL database using env file or local csv)\n",
    "### For this we chose to initailly eliminate columns with more than 50% of the values being Null, and rows with anymore than 25% Null values. We also removed 'property county land use code' and 'property zoning desc' as they could not be converted from objects. Any further nulls were imputed with zero. We also made the data more human readable by changing the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrangling data and assigning it to a dataframe\n",
    "df = w.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into three dataframes for training, validation, and testing\n",
    "train, validate, test = w.tvt_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d97bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shape of the three datasets\n",
    "w.split_view(train, validate, test)\n",
    "train_scaled = w.scale(train,['fireplace_count', 'latitude', 'longitude', 'tax_value', 'calc_sqft', 'log_error','pool_count','fips'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4b501",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d8f4e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Feature**|**Definition**|**Manipulation**|**Data Type**|\n",
    ":-|:-|:-:|:-|\n",
    "*calc_bath_and_bed*| The number of Bathrooms|Name change| float\n",
    "*calc_sqft*| The square footage of the property| Name change | float\n",
    "*fips* | The code of geographic location| Data Conversion| categorical\n",
    "*fireplace_count*|The number of fireplaces on a property| Null values imputed as zero | float\n",
    "*garage_car_count*| The number of cars a gargae can hold| Null values imputed as zero| float\n",
    "*lot_sqft*| The square footage assigned to a property's exterior| Null values imputed as zero| float\n",
    "*pool_count*| The number of pools on a property| Null values imputed as zero| float\n",
    "*tax_value*| The dollar amount paid in taxes for the property| | Null values imputed as zero| float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9b0b7",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "## Our Thought Process:\n",
    "   1. We began by taking a look at our train dataset in relation to our target vaiable, logerror.\n",
    "   2. We then look to see what individial features tend to deviate significantly from zero logerror. \n",
    "   3. From looking at our features, we were able to generate four specific questions to ask of the data. \n",
    "   4. We used our human eyes and computer algorithms to create clusters which were tested against the target variable.\n",
    "   5. We used visualizations and statistical tests to answer our questions. Sometimes the evaluation and answers lead to further questions. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff357aeb",
   "metadata": {},
   "source": [
    "## Question One: Woody\n",
    "### Can calculated square footage and tax value help predict log error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.tax_sqft_plot(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a1efc",
   "metadata": {},
   "source": [
    "### Based on this graph, we can conclude that there does appear to be a relationship between calculated square footage, tax value, and log errror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a080945",
   "metadata": {},
   "source": [
    "## Question Two: Woody\n",
    "### Does clustering based on tax value and square footage result in statistically significant differences in each cluster's log error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f2ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ac7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(e)\n",
    "train_scaled,scaler,kmeans = m.scale_and_cluster(train,['fireplace_count', 'latitude', \n",
    "    'longitude', 'tax_value', 'calc_sqft','pool_count','fips'],cluster_cols=['tax_value','calc_sqft'],cluster_name='tax_sqft',target='log_error',k=5)\n",
    "e.generate_elbow(train_scaled[['calc_sqft','tax_value']])\n",
    "train_scaled.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490ea88",
   "metadata": {},
   "source": [
    "Question: is the mean log_error of items clustered by tax_value and calc_sqft significantly different from each other?\n",
    "\n",
    "$H_0$: $\\mu_{taxsqft0} = \\mu_{taxsqft1}=$...$\\mu_{taxsqft7}$\n",
    "\n",
    "$H_a$: $\\mu_{taxsqft0} \\neq \\mu_{taxsqft1} \\neq $...$\\neq \\mu_{taxsqft7}$\n",
    "\n",
    "## Assumptions\n",
    "- Independent: Yep\n",
    "- Normal: yes, Central Limit Theorum\n",
    "- Equal variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e574b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tax_sqft_cluster'] = train_scaled.tax_sqft\n",
    "train.groupby('tax_sqft_cluster').agg('var')[['log_error']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [train[train.tax_sqft_cluster == i].log_error for i in range(5)]\n",
    "t,p = stats.levene(samples[0],samples[1],samples[2],samples[3],samples[4])\n",
    "e.p_to_md(p,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed9e8e",
   "metadata": {},
   "source": [
    "## Because the ranges of variance are so wide, we will use the non-parametric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [train[train.tax_sqft_cluster == x].log_error.to_numpy() for x in range(4)]\n",
    "t,p = stats.f_oneway(group_list[0],group_list[1],group_list[2],group_list[3])\n",
    "e.t_to_md(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517fb6e5",
   "metadata": {},
   "source": [
    "## However, given that there is such a strong cluster of these values based on tax value and square footage, we will move forward with the clusters as regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2691854",
   "metadata": {},
   "source": [
    "## Question Three:\n",
    "### Will making a cluster of what my human eyes determine to be valuable out-door features help determine houses with significantly higher or lower logerror?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ae813",
   "metadata": {},
   "source": [
    "#### Valuable out-door features are garage count, pool count, and lot size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ea606",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled3 = e.scaled_3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.elbow_for_Q3(train_scaled3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d598667",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.viz_for_Q3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.anova_test(train, 'cluster3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.anova_test(train_scaled3, 'cluster3_scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd75ea1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>  Takeaway:\n",
    "    Based on the anova test we must conceed that there is no relationship between the cluster of features and log error. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e202e",
   "metadata": {},
   "source": [
    "## Question Four:\n",
    "### Is there a relationship between features that consistently have at least one cluster around zero log error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f319b",
   "metadata": {},
   "source": [
    "#### To answer this we ran a really long loop that built four clusters around every pair of unique features and took note of when one or more clusters were centered around center of the y-axis which denoted a zero logerror. We highlighted features that reoccured and considered them for future exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.cluster_creator(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SPACE IS FOR THE T-TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8c7c1",
   "metadata": {},
   "source": [
    "THIS IS FOR THE TAKEAWAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81392f73",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aded73",
   "metadata": {},
   "source": [
    "## For modeling, we chose to give each cluster its own regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b68f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "md,_ =m.select_baseline(train.log_error)\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bdf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(m)\n",
    "tv_errors,scaler,kmeans, regressors = m.train_and_validate_errors(train,validate)\n",
    "tv_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf43dc6",
   "metadata": {},
   "source": [
    "## Linear Regression performed best, and will be model used to run on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82955a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.test_errors(test,scaler,kmeans,regressors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "653f3b7eb86bda3113d2d1f85ae6ed711e8a2906aba868c97a03afd554718a6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
