{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c25ef4",
   "metadata": {},
   "source": [
    "# Welcome to our Project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81e533",
   "metadata": {},
   "source": [
    "## Goal of This Project: \n",
    "### We are continuing working on using regression modeling to make predictions based on our Zillow data of 2017 properties for single family homes. In this itteration of the project we are using clustering algorithms to aid us in determining similarites of the data. We are also shifting our focus to logerror rather than tax value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1eaa18",
   "metadata": {},
   "source": [
    "## Project Plan\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a996d",
   "metadata": {},
   "source": [
    "### Acquire and Prepare Data\n",
    "   1. Data is brought in from either the SQL database using a locally stored env file that must contain the username, password, and host name in a get_bd_url function OR via a locally strored csv file.\n",
    "   2. Data is prepared in order for our team to explore and modified to use in modeling\n",
    "   3. Data is split into three dataframes called train, validate, and test so that we can train and explore on in-sample data and use out-of-sample to validate and finally test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfc78f",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "   1. Use visualizations to see patterns in the data and try to determine drivers/indicators of logerror\n",
    "   2. Use statistical testing, if necessary, to confirm or reject the relationship between features\n",
    "   3. Use clustering to create new categorical features that can be explored \n",
    "   4. Determine what features align with logerror deviating significantly from zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf2972",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "   1. Take features determined to be significant in exploration\n",
    "   2. Determine which regression algorithm to use\n",
    "   3. Run 4 models with chosen features and determine the best one by evaluating the RMSE score on the train set and run on the validate set\n",
    "   4. The best model will be deterined by lowest RMSE on train, and lowest variance between the RMSE of train and validate\n",
    "   5. Run the top single model on the test dataset\n",
    "   6. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ee02b",
   "metadata": {},
   "source": [
    "### Recommendations / What Comes Next\n",
    "   1. Based on model evaluation of the test dataset make recommendations and predictions\n",
    "   2. Recommendations will include...\n",
    "   3. Further steps are..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24196ffa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b79f58b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (explore.py, line 203)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3397\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m in \u001b[0;35m<cell line: 2>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import explore as e\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/codeup-data-science/clustering_exercises/clustering-project/explore.py:203\u001b[0;36m\u001b[0m\n\u001b[0;31m    kmeans = KMeans(n_clusters= random_state=89).fit(X4_scaled)\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import wrangle as w\n",
    "import explore as e\n",
    "import wrangle_Naomi as wn\n",
    "import model as m\n",
    "import env\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from importlib import reload\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99c2fd",
   "metadata": {},
   "source": [
    "## Wrangling the Data (from SQL database using env file or local csv)\n",
    "### For this we chose to initailly eliminate columns with more than 50% of the values being Null, and rows with anymore than 25% Null values. We also removed 'property county land use code' and 'property zoning desc' as they could not be converted from objects. Any further nulls were imputed with zero. We also made the data more human readable by changing the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrangling data and assigning it to a dataframe\n",
    "df = w.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into three dataframes for training, validation, and testing\n",
    "train, validate, test = w.tvt_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d97bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shape of the three datasets\n",
    "w.split_view(train, validate, test)\n",
    "train_scaled = w.scale(train,['fireplace_count', 'latitude', 'longitude', 'tax_value', 'calc_sqft', 'log_error','pool_count','fips'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4b501",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d8f4e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Feature**|**Definition**|**Manipulation**|**Data Type**|\n",
    ":-|:-|:-:|:-|\n",
    "*calc_bath_and_bed*| The number of Bathrooms|Name change| float\n",
    "*calc_sqft*| The square footage of the property| Name change | float\n",
    "*fips* | The code of geographic location| Data Conversion| categorical\n",
    "*fireplace_count*|The number of fireplaces on a property| Null values imputed as zero | float\n",
    "*garage_car_count*| The number of cars a gargae can hold| Null values imputed as zero| float\n",
    "*lot_sqft*| The square footage assigned to a property's exterior| Null values imputed as zero| float\n",
    "*pool_count*| The number of pools on a property| Null values imputed as zero| float\n",
    "*tax_value*| The dollar amount paid in taxes for the property| | Null values imputed as zero| float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9b0b7",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "## Our Thought Process:\n",
    "   1. We began by taking a look at our train dataset in relation to our target vaiable, logerror.\n",
    "   2. We then look to see what individial features tend to deviate significantly from zero logerror. \n",
    "   3. From looking at our features, we were able to generate four specific questions to ask of the data. \n",
    "   4. We used our human eyes and computer algorithms to create clusters which were tested against the target variable.\n",
    "   5. We used visualizations and statistical tests to answer our questions. Sometimes the evaluation and answers lead to further questions. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdcc4f0",
   "metadata": {},
   "source": [
    "## Question One:\n",
    "### Is there a relationship between features that consistently have at least one cluster around zero log error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566e5ea",
   "metadata": {},
   "source": [
    "#### To answer this we ran a really long loop that built four clusters around every pair of unique features and took note of when one or more clusters were centered around center of the y-axis which denoted a zero logerror. We highlighted features that reoccured and considered them for future exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b998965",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled4 = e.scaled_4(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.cluster_creator(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1519376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hopefully woody can create a function, or fix yvettes function, to loop t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff357aeb",
   "metadata": {},
   "source": [
    "## Question Two:\n",
    "### Can calculated square footage and tax value help predict log error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.tax_sqft_plot(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a1efc",
   "metadata": {},
   "source": [
    "### Based on this graph, we can conclude that there does appear to be a relationship between calculated square footage, tax value, and log errror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a080945",
   "metadata": {},
   "source": [
    "## Question Three: \n",
    "### Does clustering based on tax value and square footage result in statistically significant differences in each cluster's log error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ac7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(e)\n",
    "train_scaled,scaler,kmeans = m.scale_and_cluster(train,['fireplace_count', 'latitude', \n",
    "    'longitude', 'tax_value', 'calc_sqft','pool_count','fips'],cluster_cols=['tax_value','calc_sqft'],cluster_name='tax_sqft',target='log_error',k=5)\n",
    "e.generate_elbow(train_scaled[['calc_sqft','tax_value']])\n",
    "train_scaled.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97270c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.tax_sqft_cluster_plot(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490ea88",
   "metadata": {},
   "source": [
    "# Question: is the mean log_error of items clustered by tax_value and calc_sqft significantly different from each other?\n",
    "\n",
    "$H_0$: $\\mu_{taxsqft0} = \\mu_{taxsqft1}=$...$\\mu_{taxsqft7}$\n",
    "\n",
    "$H_a$: $\\mu_{taxsqft0} \\neq \\mu_{taxsqft1} \\neq $...$\\neq \\mu_{taxsqft7}$\n",
    "\n",
    "## Assumptions\n",
    "- Independent: Yep\n",
    "- Normal: yes, Central Limit Theorum\n",
    "- Equal variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e574b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tax_sqft_cluster'] = train_scaled.tax_sqft\n",
    "train.groupby('tax_sqft_cluster').agg('var')[['log_error']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [train[train.tax_sqft_cluster == i].log_error for i in range(5)]\n",
    "t,p = stats.levene(samples[0],samples[1],samples[2],samples[3],samples[4])\n",
    "e.p_to_md(p,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf35cd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: </b> Because the ranges of variance are so wide, we will use the non-parametric test\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [train[train.tax_sqft_cluster == x].log_error.to_numpy() for x in range(4)]\n",
    "t,p = stats.f_oneway(group_list[0],group_list[1],group_list[2],group_list[3])\n",
    "e.t_to_md(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9c186",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Moving Forward</b> However, given that there is such a strong cluster of these values based on tax value and square footage, we will move forward with the clusters as regression models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2691854",
   "metadata": {},
   "source": [
    "## Question Four:\n",
    "### Will making a cluster of what my human eyes determine to be valuable out-door features help determine houses with significantly higher or lower logerror?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ae813",
   "metadata": {},
   "source": [
    "#### Valuable out-door features are garage count, pool count, and lot size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ea606",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled3 = e.scaled_3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.elbow_for_Q3(train_scaled3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d598667",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.viz_for_Q3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.anova_test(train, 'cluster3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.anova_test(train_scaled3, 'cluster3_scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd75ea1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>  Takeaway:\n",
    "    Based on the anova test we must conceed that there is no relationship between the cluster of features and log error, and that my human intuition is useless. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81392f73",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a080870",
   "metadata": {},
   "source": [
    "### Moving forward in our modeling we chose to use tax value and calculated square feet as a clustered feature, along with the features fireplace count, latitude, and longitude. We chose to include these in our model because all six features showed to consistently coincide with having clusters that were both centered around zero logerror as well as having wide variance away from zero logerror. We chose to use tax value and calculated square feet as a cluster because together they showed the most centralized cluster around zero with the fewest significant outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33d880",
   "metadata": {},
   "source": [
    "## Modeling Pre-Processing\n",
    "   1. First we decided what features would move into our model.\n",
    "      * we chose to use tax value and calculated square feet as a clustered feature, along with the features fireplace count, latitude, and longitude.\n",
    "      * We chose to use tax value and calculated square feet as a cluster because together they showed the most centralized cluster around zero with the fewest significant outliers.\n",
    "   2. We ensured all of our data was numerical\n",
    "   3. We split our train, validate, and test data sets to seperate the target variable of logerror to use as y- train, validate, and test.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a486fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "md,baseline = m.select_baseline(train.log_error)\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a06a29",
   "metadata": {},
   "source": [
    "### We Used 3 Regression Models on Train and Validate To Determine Which to Use on Our Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab25b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_errors,scaler,kmeans,regressors = m.train_and_validate_errors(train,validate)\n",
    "pd.concat([baseline,train_validate_errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edde2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Evaluation:</b> Based on the resulting RMSE between the train and validate sets, we decided to use Linear Regression on our test set because the train RMSE is lower than baseline, and the difference between the train and validate does not hint at overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.test_errors(test,scaler,kmeans,regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8e329",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Conclusion:</b> Our model run on the test data results in RMSE score that is \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b674568",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26cf8491",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4032f221",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ca9870a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08003cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa6a981a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee667cab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78aeddfd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "653f3b7eb86bda3113d2d1f85ae6ed711e8a2906aba868c97a03afd554718a6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
